# -*- coding: utf-8 -*-
"""Regression de Cox Cyclic Coordiante Descent.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1F5mRwLrNk-shK41GCPfGoui2YsJQcAAH
"""

from google.colab import drive
drive.mount('/content/drive')

import os

"""Donéées regression de Cox et calcul de la dérivé de la Negative Log-Likelihood
https://www.kaggle.com/datasets/wordsforthewise/lending-club?resource=download 
Donnée provenant de All Lending Club 2007 à 2018 sur [Kaggle](https://www.kaggle.com/datasets/wordsforthewise/lending-club?resource=download ) : En tout un million ̃données. Le data set retenu pour le training et le test est de 30 000 données.
$beta_i$


"""



my_seed = 0
import numpy as np
np.random.seed(my_seed)
import random
random.seed(my_seed)
import pandas as pd
from sklearn.preprocessing import StandardScaler,MinMaxScaler


precision=10**-15

import re

path="./drive/My Drive/Cox_Screening/loan.csv"
general_data=pd.read_csv(path,sep=',')
general_data.info()

"""Code du Cyclic Coordinate Bound """

def CCB(Limite,fonction,indice,beta_prime): # fonction qui calcule les limites L et U de beta_prime(indice)
  
  L=np.copy(beta_prime)
  U=np.copy(beta_prime)
  #L[indice]=-1*abs(np.random.rand())
  #U[indice]=abs(np.random.rand())
  L[indice]=-1
  U[indice]=1

  lim=(U[indice])-(L[indice])
  f_L=fonction(L)
  f_U=fonction(U)
  while((lim<Limite) and ((f_L>=0) or (f_U<=0))):
    if (f_L>=0):
      L[indice]*=1.15
      f_L=fonction(L)
      
      
    if (f_U<=0):
      U[indice]*=1.15
      f_U=fonction(U)
    
    lim=U[indice]-L[indice]
    
  return(L,U)

"""Fonction du Cyclic Coordinate Descent"""

def CCD(error_max,Iter_Max,Limite_ini,fonction,size):
  beta=np.zeros((size,1))
  error=fonction(beta)
  i=j=k=0
  while((i<Iter_Max) and abs(error)>error_max):
    #print(i)
    liste=list(range(size))
    random.shuffle(liste)
    for j in liste:
        L,U=CCB(Limite,fonction,j,beta)
        #print("iteration ",i,"indice",j,"L:",L,"U",U,"\n")
        ecart=U[j]-L[j]
        while((k<Iter_Max)and ecart>precision):
          beta[j]=(L[j]+U[j])/2
          error=fonction(beta)
          if error<0 :
            L=np.copy(beta)
          else : 
            U=np.copy(beta)
          ecart=U[j]-L[j]
          k+=1
    i+=1
  return (beta,abs(error))

def CCDF(error_max,Iter_Max,Limite_ini,fonction,size,flags):
  beta=np.zeros((size,1))
  #error=fonction(beta)
  i=j=k=0
  error=10
  while((i<Iter_Max) and abs(error)>error_max):
    #print(i)
    liste=list(range(size))
    random.shuffle(liste)
    for j in liste:
        if flags[j]==1:
          break;
        fonc=lambda x : fonction(x,j)# derivé sur la j-ième composante de beta
        L,U=CCB(Limite,fonc,j,beta)
        print("calcul des bounds l'itération {}  de l'indice {} donne (L,U) :({},{})\n".format(i,j,L[j],U[j]))
        #print("iteration ",i,"indice",j,"L:",L,"U",U,"\n")
        ecart=U[j]-L[j]
        k=0
        while((k<Iter_Max)and ecart>precision):
          beta[j]=(L[j]+U[j])/2
          error=fonc(beta)
          print("calcul de l'erreur de l'itération {} de l'indice {} donne :{}\n".format(i,j,error))
          if error<0 :
            L=np.copy(beta)
          else : 
            U=np.copy(beta)
          ecart=U[j]-L[j]
          k+=1
          print("calcul du nouveau beta de  l'itération {} l'indice {} donne :{}\n".format(i,j,beta))
    i+=1
  return (beta)

"""Fonctions de test"""

def f1(x):
  return 2*x[0]**3-8

def f2(x):
  return x[0]**2+2*x[1]*2+1+2*x[1]*x[0]**1

def f3(x):
  return x[0]+2*x[1]+2*x[0]*x[2]+1

Limite=20
fonction=f2
indice=1
beta_prime=np.array([2,1],dtype=float)
CCB(Limite,fonction,indice,beta_prime)

Limite=10
fonction=f1
indice=0
beta_prime=np.array([1],dtype=float)
CCB(Limite,fonction,indice,beta_prime)

error_max=0.000001
Iter_Max=5000
Limite_ini=500
fonction=f2
size=2
CCD(error_max,Iter_Max,Limite_ini,fonction,size)

CCD(error_max,Iter_Max,Limite_ini,f3,3)

general_data.info()

general_data.select_dtypes(include="object").columns

general_data[['last_pymnt_d','issue_d','loan_status']].head()

def date(date):
  months=["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"]
  times=[]
  for line in list(date):
    i=line[0]
    j=line[1]
    split1=str(i).split("-")
    split2=str(j).split("-")
    time=(months.index(split1[0])+1)*30+int(split1[1])*364
    time=time-((months.index(split2[0])+1)*30+int(split2[1])*364)
    times.append(time)
  return times

features=["annual_inc","dti","emp_length","funded_amnt","grade","home_ownership","int_rate","loan_amnt","mort_acc","num_bc_sats","num_bc_tl","pub_rec_bankruptcies","revol_bal","term",'last_pymnt_d','issue_d','loan_status']
data=general_data[features]
data=data.drop_duplicates()
data=data.dropna()
times=date(data[['last_pymnt_d','issue_d']].values)
data["Time"]=times
data["Death"]=0
data.Death[data.loan_status=="Charged Off"]=1
data=data.drop(['last_pymnt_d','issue_d','loan_status'],axis=1)
#data.info()

data.head()

expr1="\d{1,2}"
temp=[]
for line in data.emp_length.values:
  temp.append(int(re.findall(expr1,str(line))[0]))
data.emp_length=temp
temp=[]
for line in data.term.values:
  temp.append(int(re.findall(expr1,str(line))[0]))
data.term=temp

data.info()

data=pd.get_dummies(data,columns=["grade","home_ownership","term"],drop_first=False,dtype=int)
data_columns=['annual_inc', 'dti', 'emp_length', 'funded_amnt', 'int_rate',
       'loan_amnt', 'mort_acc', 'num_bc_sats', 'num_bc_tl',
       'pub_rec_bankruptcies', 'revol_bal', 'grade_A', 'grade_B', 'grade_C', 'grade_D', 'grade_E', 'grade_F', 'grade_G',
       'home_ownership_ANY', 'home_ownership_MORTGAGE', 'home_ownership_OWN',
       'home_ownership_RENT', 'term_36', 'term_60','Time', 'Death']
data=data[data_columns]
data.info()

data_set=data.iloc[:200,:].copy()
Lasso=1

feature_to_scale=["annual_inc","dti","emp_length","funded_amnt","int_rate","loan_amnt","mort_acc","num_bc_sats","num_bc_tl","pub_rec_bankruptcies","revol_bal"]
print(data_set[feature_to_scale].describe())
StandardScaling=MinMaxScaler().fit(data_set[feature_to_scale].values)#MinMaxScaler
data_set[feature_to_scale]=StandardScaling.transform(data_set[feature_to_scale])
data_set[feature_to_scale].describe()

from numpy.core.fromnumeric import shape
def Derivate(betas):
  dataD=data_set.sort_values(by=["Death","Time"],axis=0)
  death_time=dataD[dataD.Death==1].Time.unique()
  grad=np.zeros_like(betas)
  
  
  for  k in range(len(betas)):
    l=0
    beta=betas[k]

    for t_i in death_time:
      sum1=0
      sum2=0
      sum3=0
      I_i=dataD[(dataD["Death"]==1) & (dataD["Time"]==t_i)].iloc[:,:-2].values #  I(i)
      #print("I_i shape",I_i.shape,"\n")
      R_i=dataD[(dataD["Death"]==0) | (dataD["Time"]>=t_i)].iloc[:,:-2].values #  R(i)
      #print("R_i shape",R_i.shape,"\n")
      d_i=I_i.shape[0]
      for x_s in I_i:
        sum1+=x_s[k]
      

      for x_j in R_i:
        #print("x_j",x_j.shape,"\n")
        sum2+=x_j[k]*np.exp(x_j@betas)
        sum3+=np.exp(x_j@betas)
      l+=sum1-(d_i/sum3)*sum2
    if beta>0:
      l+=Lasso
    else :
      l-=Lasso
    grad[k]=l
  return(grad)

import math

dataD_out=data_set.sort_values(by=["Death","Time"],axis=0)
death_time=dataD_out[dataD_out.Death==1].Time.unique()
R=[]
I=[]
d=[]
for t_i in death_time:
    I_i=dataD_out[(dataD_out["Death"]==1) & (dataD_out["Time"]==t_i)].iloc[:,:-2].values #  I(i)
    #print("I_i shape",I_i.shape,"\n")
    R_i=dataD_out[(dataD_out["Death"]==0) | (dataD_out["Time"]>=t_i)].iloc[:,:-2].values #  R(i)
    #print("R_i shape",R_i.shape,"\n")
    d_i=I_i.shape[0]
    I.append(I_i)
    R.append(R_i)
    d.append(d_i)

def Derivate_unique(betas,indice):
  dataD=data_set.sort_values(by=["Death","Time"],axis=0)
  death_time=dataD[dataD.Death==1].Time.unique()
  grad=np.zeros(1)
  l=0
  beta=betas[indice]

  for t_i in death_time:
    sum1=0
    sum2=0
    sum3=0
    I_i=dataD[(dataD["Death"]==1) & (dataD["Time"]==t_i)].iloc[:,:-2].values #  I(i)
    #print("I_i shape",I_i.shape,"\n")
    R_i=dataD[(dataD["Death"]==0) | (dataD["Time"]>=t_i)].iloc[:,:-2].values #  R(i)
    #print("R_i shape",R_i.shape,"\n")
    d_i=I_i.shape[0]
    
    for x_s in I_i:
      sum1+=x_s[indice]
      

    for x_j in R_i:
      #print("x_j",x_j.shape,"\n")
      sum2+=x_j[indice]*np.exp(x_j@betas)
      sum3+=np.exp(x_j@betas)
    l+=sum1-(d_i/sum3)*sum2
  if beta>0:
    l+=Lasso
  else :
    l-=Lasso
  if (math.isinf(l)) or (np.isnan(l)):
    print("calcul de la dérivée de l'indice {} donne {}\n".format(indice,l))
  return(l)

betas=np.ones(24)*1
Derivate(betas)

Derivate_unique(betas,7)

error_max=1
Iter_Max=10
Limite_ini=3
fonction=Derivate_unique
size=24
flags=np.zeros(size)

array=CCDF(error_max,Iter_Max,Limite_ini,fonction,size,flags)

Derivate(array)

dataD=data_set.sort_values(by=["Death","Time"],axis=0)
dataD[(dataD["Death"]==1) & (dataD["Time"]==244)].iloc[:,:-2].values.shape[0]

dataD=data_set.sort_values(by=["Death","Time"],axis=0)
dataD[dataD.Death==1].Time.unique()

I=dataD[(dataD["Death"]==0) | (dataD["Time"]>=1186)].iloc[:,:-2].values
I.shape

dataD=data_set.sort_values(by=["Death","Time"],axis=0)
dataD[["Death","Time"]]

dataD[dataD["Death"]==1].groupby(["Time"]).describe()